{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Workshop 1: Pandas and Python.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lVakoZ75MQTA"},"source":["# How to Survive an Encounter with Pandas and Python. "]},{"cell_type":"code","metadata":{"id":"fJvRJBLvggjd","cellView":"form"},"source":["#@title Prep Code - Please Disregard! \n","cc_institution_grads = pd.read_csv('cc_institution_grads.csv', encoding = \"ISO-8859-1\") # Adjust File Path As Needed 'cc_institution_grads.csv'\n","# Dataset is equally distributed across universities\n","\n","grads = cc_institution_grads[['unitid', 'gender', 'race']]\n","\n","gender_counts=grads.groupby(['unitid', 'gender']).count().reset_index()\n","gender_counts = gender_counts.rename(columns = {\"race\":\"gendercount\"})\n","gender_counts = gender_counts.pivot(index='unitid', columns='gender', values='gendercount').reset_index()\n","gender_counts = gender_counts.rename(columns = {\"F\":\"Female\",\n","                                                \"M\":\"Male\",\n","                                                \"B\":\"Other\"})\n","\n","\n","race_counts=grads.groupby(['unitid', 'race']).count().reset_index()\n","race_counts = race_counts.rename(columns = {\"gender\":\"racecount\"})\n","race_counts = race_counts.pivot(index='unitid', columns='race', values='racecount').reset_index()\n","race_counts = race_counts.rename(columns = {\"A\":\"Asian\", \"Ai\": \"American Indian\",\n","                                            \"B\":\"Black\", \"H\": \"Hispanic\",\n","                                            \"W\":\"White\", \"X\":\"Unknown\"})\n","\n","\n","grad_details = gender_counts.merge(race_counts, how='inner', on=['unitid'], copy=False)\n","\n","grad_details.to_csv('grad_details.csv', index=False) #Adjust File Path As Needed 'grad_details.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YteAsMZXF0We"},"source":["#Introduction\n","\n","## What is Data Science?\n","\n","Data Science is the discipline of reading and manipulating data to extract meaningful insights. The field is one of the most rapidly growing today and has applications in fields like Finance, Healthcare, and Natural Sciences, not to mention its growth in industry. IBM has projected a 2.7 million job growth in Data Science and Data Analytics (https://www.ibm.com/downloads/cas/3RL3VXGA) and the Bureau of Labor and Statistics considers data science one of the top 20 fastest growing occupations (https://www.bls.gov/ooh/fastest-growing.htm). \n","\n","## What is Python?\n","\n","Python is a high-level programming language that is used in data science projects due to its clear, concise nature and the numerous libraries/frameworks available to its users. Among these is <b>Pandas</b>. \n","\n","Linked below is a cheatsheet of python commands for anyone who needs to brush up on their skills:\n","http://datacamp-community-prod.s3.amazonaws.com/0eff0330-e87d-4c34-88d5-73e80cb955f2\n","\n","## What is Pandas?\n","\n","Pandas is a data analysis tool that allows for easy import and manipulation of data. It's really easy to use, and we'll cover the basics of how to get Pandas up and running. If you have familiarity with Excel, a lot of Pandas concepts should seem familiar. \n"]},{"cell_type":"code","metadata":{"id":"kLrAAQhDKorz"},"source":["# Importing the Pandas Library\n","\n","import pandas as pd # Labeling this as pd is a standard practice. "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nctZbSQPJY-S"},"source":["# Dataframes\n","\n","The basic structure for storing data in pandas is known as a <b>dataframe</b>. You can think of dataframes as tables of data that you can then start playing with and manipulating. To create a dataframe, we can use a similar structure as a dictionary. Each column name can be considered a sort of key, while the observations within that column can be viewed as values. Pandas takes this structure and interprets it as a dataframe, which is a data type in and of itself. \n"]},{"cell_type":"code","metadata":{"id":"c-tDzBIgLWRO","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1631039661690,"user_tz":240,"elapsed":374,"user":{"displayName":"John Kirollos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXlpWNgQGE88mhrSgUJB6Dx3RLSlEhHifAZALdwg=s64","userId":"10752091771659842492"}},"outputId":"a78fd4bb-09ac-488a-f7ca-8a6d06c07762"},"source":["# Inititalize a new 3x3 Dataframe\n","df = pd.DataFrame({\"Col1\": [1, 2, 3],\n","                  \"Col2\": [4, 5, 6],\n","                  \"Col3\": [7, 8, 9]})\n","\n","# Print dataframe\n","print(\"This is the printed dataframe:\")\n","print(df)\n","\n","# Just showing the dataframe\n","print(\"\\nThis is a called dataframe:\")\n","df"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This is the printed dataframe:\n","   Col1  Col2  Col3\n","0     1     4     7\n","1     2     5     8\n","2     3     6     9\n","\n","This is a called dataframe:\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Col1</th>\n","      <th>Col2</th>\n","      <th>Col3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>6</td>\n","      <td>9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Col1  Col2  Col3\n","0     1     4     7\n","1     2     5     8\n","2     3     6     9"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"ZsdlyMyJL1EI"},"source":["# Reading in External Data\n","\n","Often, you will be working with data that is already available. With this, you will need to know how to import data and how to begin working with it. For today's lesson, we will be using two datasets. One consists of information on a number of universities and the other contains information about the types of students at each institution. \n","\n","The datasets can be found here: [GitHub](https://github.com/CarolinaDataScience/Workshops2021/tree/main/An%20Encounter%20With%20Pythons%20and%20Pandas). Please download **cc_institution_details.cvs** and **grad_details.csv** and put them into your Google Drive. Then copy the file path for each of those files and paste it in the parentheses below. The areas are marked. \n","\n","---\n","While we are using slightly modified files for today's case, the original datasets can be found here: https://data.world/databeats/college-completion. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVIIA3sZU5RA","executionInfo":{"status":"ok","timestamp":1631039246169,"user_tz":240,"elapsed":25476,"user":{"displayName":"John Kirollos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXlpWNgQGE88mhrSgUJB6Dx3RLSlEhHifAZALdwg=s64","userId":"10752091771659842492"}},"outputId":"a5af0a57-4035-4f7f-fb75-d543f6c0286f"},"source":["# You may need to run this cell to connect the notebook to your individual Google Drive. \n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"1lhvBXLlL0F2"},"source":["# Loading in the actual data. Please input the direct path from your drive. \n","cc_institution_details = pd.read_csv('/###Adjust Path Here###/', encoding = \"ISO-8859-1\") # Adjust File Path As Needed!\n","cc_institution_grads = pd.read_csv('/###Adjust Path Here###/') # Adjust File Path As Needed!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gCqzXbrN3_Ji"},"source":["# Quick Look At The Data\n","\n","Once the data is loaded in, it is good to take a look at a snapshot of the dataframe. To do this, we use the function **head()**."]},{"cell_type":"code","metadata":{"id":"_0mpx-gHXeTa","colab":{"base_uri":"https://localhost:8080/","height":503},"executionInfo":{"status":"ok","timestamp":1631039653243,"user_tz":240,"elapsed":294,"user":{"displayName":"John Kirollos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXlpWNgQGE88mhrSgUJB6Dx3RLSlEhHifAZALdwg=s64","userId":"10752091771659842492"}},"outputId":"dd9b742f-6b93-4132-a63e-397a42bfd539"},"source":["cc_institution_details.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>unitid</th>\n","      <th>chronname</th>\n","      <th>city</th>\n","      <th>state</th>\n","      <th>level</th>\n","      <th>control</th>\n","      <th>basic</th>\n","      <th>hbcu</th>\n","      <th>flagship</th>\n","      <th>long_x</th>\n","      <th>lat_y</th>\n","      <th>site</th>\n","      <th>student_count</th>\n","      <th>awards_per_value</th>\n","      <th>awards_per_state_value</th>\n","      <th>awards_per_natl_value</th>\n","      <th>exp_award_value</th>\n","      <th>exp_award_state_value</th>\n","      <th>exp_award_natl_value</th>\n","      <th>exp_award_percentile</th>\n","      <th>ft_pct</th>\n","      <th>fte_value</th>\n","      <th>fte_percentile</th>\n","      <th>med_sat_value</th>\n","      <th>med_sat_percentile</th>\n","      <th>aid_value</th>\n","      <th>aid_percentile</th>\n","      <th>endow_value</th>\n","      <th>endow_percentile</th>\n","      <th>grad_100_value</th>\n","      <th>grad_100_percentile</th>\n","      <th>grad_150_value</th>\n","      <th>grad_150_percentile</th>\n","      <th>pell_value</th>\n","      <th>pell_percentile</th>\n","      <th>retain_value</th>\n","      <th>retain_percentile</th>\n","      <th>ft_fac_value</th>\n","      <th>ft_fac_percentile</th>\n","      <th>vsa_year</th>\n","      <th>vsa_grad_after4_first</th>\n","      <th>vsa_grad_elsewhere_after4_first</th>\n","      <th>vsa_enroll_after4_first</th>\n","      <th>vsa_enroll_elsewhere_after4_first</th>\n","      <th>vsa_grad_after6_first</th>\n","      <th>vsa_grad_elsewhere_after6_first</th>\n","      <th>vsa_enroll_after6_first</th>\n","      <th>vsa_enroll_elsewhere_after6_first</th>\n","      <th>vsa_grad_after4_transfer</th>\n","      <th>vsa_grad_elsewhere_after4_transfer</th>\n","      <th>vsa_enroll_after4_transfer</th>\n","      <th>vsa_enroll_elsewhere_after4_transfer</th>\n","      <th>vsa_grad_after6_transfer</th>\n","      <th>vsa_grad_elsewhere_after6_transfer</th>\n","      <th>vsa_enroll_after6_transfer</th>\n","      <th>vsa_enroll_elsewhere_after6_transfer</th>\n","      <th>similar</th>\n","      <th>state_sector_ct</th>\n","      <th>carnegie_ct</th>\n","      <th>counted_pct</th>\n","      <th>nicknames</th>\n","      <th>cohort_size</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>100654</td>\n","      <td>Alabama A&amp;M University</td>\n","      <td>Normal</td>\n","      <td>Alabama</td>\n","      <td>4-year</td>\n","      <td>Public</td>\n","      <td>Masters Colleges and Universities--larger prog...</td>\n","      <td>X</td>\n","      <td>NaN</td>\n","      <td>-86.568502</td>\n","      <td>34.783368</td>\n","      <td>www.aamu.edu/</td>\n","      <td>4051</td>\n","      <td>14.2</td>\n","      <td>18.8</td>\n","      <td>21.5</td>\n","      <td>105331</td>\n","      <td>75743</td>\n","      <td>66436</td>\n","      <td>90</td>\n","      <td>93.8</td>\n","      <td>3906</td>\n","      <td>33</td>\n","      <td>823.0</td>\n","      <td>0.0</td>\n","      <td>7142.0</td>\n","      <td>72.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>10.0</td>\n","      <td>15.0</td>\n","      <td>29.1</td>\n","      <td>14.0</td>\n","      <td>71.2</td>\n","      <td>98.0</td>\n","      <td>63.1</td>\n","      <td>17.0</td>\n","      <td>82.8</td>\n","      <td>89.0</td>\n","      <td>2010.0</td>\n","      <td>14.7</td>\n","      <td>2.0</td>\n","      <td>36.5</td>\n","      <td>16.1</td>\n","      <td>33.0</td>\n","      <td>5.3</td>\n","      <td>12.5</td>\n","      <td>14.6</td>\n","      <td>15.7</td>\n","      <td>1.5</td>\n","      <td>40.9</td>\n","      <td>17.2</td>\n","      <td>36.4</td>\n","      <td>5.6</td>\n","      <td>17.2</td>\n","      <td>11.1</td>\n","      <td>232937|100724|405997|113607|139533|144005|2285...</td>\n","      <td>13</td>\n","      <td>386</td>\n","      <td>99.7|07</td>\n","      <td>NaN</td>\n","      <td>882.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>100663</td>\n","      <td>University of Alabama at Birmingham</td>\n","      <td>Birmingham</td>\n","      <td>Alabama</td>\n","      <td>4-year</td>\n","      <td>Public</td>\n","      <td>Research Universities--very high research acti...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>-86.809170</td>\n","      <td>33.502230</td>\n","      <td>www.uab.edu</td>\n","      <td>11502</td>\n","      <td>20.9</td>\n","      <td>18.8</td>\n","      <td>21.5</td>\n","      <td>136546</td>\n","      <td>75743</td>\n","      <td>66436</td>\n","      <td>97</td>\n","      <td>72.7</td>\n","      <td>10032</td>\n","      <td>67</td>\n","      <td>1146.0</td>\n","      <td>84.0</td>\n","      <td>6088.0</td>\n","      <td>50.0</td>\n","      <td>24136.0</td>\n","      <td>93.0</td>\n","      <td>29.4</td>\n","      <td>67.0</td>\n","      <td>53.5</td>\n","      <td>66.0</td>\n","      <td>35.1</td>\n","      <td>39.0</td>\n","      <td>80.2</td>\n","      <td>70.0</td>\n","      <td>92.4</td>\n","      <td>98.0</td>\n","      <td>2011.0</td>\n","      <td>22.3</td>\n","      <td>2.9</td>\n","      <td>34.2</td>\n","      <td>19.2</td>\n","      <td>42.6</td>\n","      <td>10.5</td>\n","      <td>7.9</td>\n","      <td>13.1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>196060|180461|201885|145600|209542|236939|1268...</td>\n","      <td>13</td>\n","      <td>106</td>\n","      <td>56.0|07</td>\n","      <td>UAB</td>\n","      <td>1376.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>100690</td>\n","      <td>Amridge University</td>\n","      <td>Montgomery</td>\n","      <td>Alabama</td>\n","      <td>4-year</td>\n","      <td>Private not-for-profit</td>\n","      <td>Baccalaureate Colleges--Arts &amp; Sciences</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>-86.174010</td>\n","      <td>32.362609</td>\n","      <td>www.amridgeuniversity.edu</td>\n","      <td>322</td>\n","      <td>29.9</td>\n","      <td>17.8</td>\n","      <td>22.5</td>\n","      <td>58414</td>\n","      <td>92268</td>\n","      <td>101725</td>\n","      <td>30</td>\n","      <td>62.7</td>\n","      <td>294</td>\n","      <td>12</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>2540.0</td>\n","      <td>1.0</td>\n","      <td>302.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>66.7</td>\n","      <td>72.0</td>\n","      <td>68.4</td>\n","      <td>91.0</td>\n","      <td>37.5</td>\n","      <td>2.0</td>\n","      <td>67.2</td>\n","      <td>71.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>217925|441511|205124|247825|197647|221856|1353...</td>\n","      <td>16</td>\n","      <td>252</td>\n","      <td>100.0|07</td>\n","      <td>NaN</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>100706</td>\n","      <td>University of Alabama at Huntsville</td>\n","      <td>Huntsville</td>\n","      <td>Alabama</td>\n","      <td>4-year</td>\n","      <td>Public</td>\n","      <td>Research Universities--very high research acti...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>-86.638420</td>\n","      <td>34.722818</td>\n","      <td>www.uah.edu</td>\n","      <td>5696</td>\n","      <td>20.9</td>\n","      <td>18.8</td>\n","      <td>21.5</td>\n","      <td>64418</td>\n","      <td>75743</td>\n","      <td>66436</td>\n","      <td>61</td>\n","      <td>74.4</td>\n","      <td>5000</td>\n","      <td>40</td>\n","      <td>1180.0</td>\n","      <td>89.0</td>\n","      <td>6647.0</td>\n","      <td>63.0</td>\n","      <td>11502.0</td>\n","      <td>81.0</td>\n","      <td>16.5</td>\n","      <td>34.0</td>\n","      <td>48.4</td>\n","      <td>54.0</td>\n","      <td>32.8</td>\n","      <td>32.0</td>\n","      <td>81.0</td>\n","      <td>72.0</td>\n","      <td>65.5</td>\n","      <td>56.0</td>\n","      <td>2010.0</td>\n","      <td>12.8</td>\n","      <td>4.7</td>\n","      <td>42.8</td>\n","      <td>18.3</td>\n","      <td>43.0</td>\n","      <td>14.5</td>\n","      <td>10.2</td>\n","      <td>11.7</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>232186|133881|196103|196413|207388|171128|1900...</td>\n","      <td>13</td>\n","      <td>106</td>\n","      <td>43.1|07</td>\n","      <td>UAH</td>\n","      <td>759.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>100724</td>\n","      <td>Alabama State University</td>\n","      <td>Montgomery</td>\n","      <td>Alabama</td>\n","      <td>4-year</td>\n","      <td>Public</td>\n","      <td>Masters Colleges and Universities--larger prog...</td>\n","      <td>X</td>\n","      <td>NaN</td>\n","      <td>-86.295677</td>\n","      <td>32.364317</td>\n","      <td>www.alasu.edu/email/index.aspx</td>\n","      <td>5356</td>\n","      <td>11.6</td>\n","      <td>18.8</td>\n","      <td>21.5</td>\n","      <td>132407</td>\n","      <td>75743</td>\n","      <td>66436</td>\n","      <td>96</td>\n","      <td>91.0</td>\n","      <td>5035</td>\n","      <td>41</td>\n","      <td>830.0</td>\n","      <td>1.0</td>\n","      <td>7256.0</td>\n","      <td>74.0</td>\n","      <td>13202.0</td>\n","      <td>84.0</td>\n","      <td>8.8</td>\n","      <td>11.0</td>\n","      <td>25.2</td>\n","      <td>9.0</td>\n","      <td>82.7</td>\n","      <td>100.0</td>\n","      <td>62.2</td>\n","      <td>15.0</td>\n","      <td>67.0</td>\n","      <td>58.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>100654|232937|242617|243197|144005|241739|2354...</td>\n","      <td>13</td>\n","      <td>386</td>\n","      <td>88.0|07</td>\n","      <td>ASU</td>\n","      <td>1351.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   unitid                            chronname  ... nicknames cohort_size\n","0  100654               Alabama A&M University  ...       NaN       882.0\n","1  100663  University of Alabama at Birmingham  ...       UAB      1376.0\n","2  100690                   Amridge University  ...       NaN         3.0\n","3  100706  University of Alabama at Huntsville  ...       UAH       759.0\n","4  100724             Alabama State University  ...       ASU      1351.0\n","\n","[5 rows x 62 columns]"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"VODHEQSq4SVI"},"source":["### Try it on your own!\n","\n","Try to show the head of our second dataframe **cc_institution_grads**."]},{"cell_type":"code","metadata":{"id":"pDBAbqR547KV"},"source":["# Please enter your solution here."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QXB7Fg6XXbnC","cellView":"form"},"source":["#@title Solution is hidden. Double click to see it.\n","\n","cc_institution_grads.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MakFWz-58hqT"},"source":["# Checking Data Types\n","\n","After we load in the data, and get a quick view of it, we should check the data types present in the dataframe. To do this we use the dtypes element that comes within Pandas. "]},{"cell_type":"code","metadata":{"id":"UH_4mvXE8izq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631040016200,"user_tz":240,"elapsed":248,"user":{"displayName":"John Kirollos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXlpWNgQGE88mhrSgUJB6Dx3RLSlEhHifAZALdwg=s64","userId":"10752091771659842492"}},"outputId":"3502f72d-d1bf-4ff4-9391-e4356639cd32"},"source":["# Checking data types. \n","cc_institution_details.dtypes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["unitid               int64\n","chronname           object\n","city                object\n","state               object\n","level               object\n","                    ...   \n","state_sector_ct      int64\n","carnegie_ct          int64\n","counted_pct         object\n","nicknames           object\n","cohort_size        float64\n","Length: 62, dtype: object"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"Rml5vXd18lRy"},"source":["### Try it on your own!\n","\n","Try to identify the data types in our second dataframe **cc_institution_grads**."]},{"cell_type":"code","metadata":{"id":"UwpZOlyuDiZX"},"source":["# Please enter your solution here."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EPRcz9e18icR","cellView":"form"},"source":["#@title Solution is hidden. Double click to see it.\n","\n","cc_institution_grads.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gbxC4l0NTdFo"},"source":["# Selecting Columns Of Interest\n","\n","It is good practice to narrow down your dataset according to the needs of your project. This keeps outside information more safely maintained and is a more ethical approach to data science. First, we will use **columns** to see the columns in the dataset. From here, we can then select certain columns that we will use in our analyses. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"184rCtH_UCR9","executionInfo":{"status":"ok","timestamp":1630963301013,"user_tz":240,"elapsed":206,"user":{"displayName":"Jonathan Schlosser","photoUrl":"","userId":"03711695131991659126"}},"outputId":"e4a6822b-2a1d-42e7-efcc-a60e66de47d9"},"source":["# Viewing the columns. \n","cc_institution_details.columns"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['unitid', 'chronname', 'city', 'state', 'level', 'control', 'basic',\n","       'hbcu', 'flagship', 'long_x', 'lat_y', 'site', 'student_count',\n","       'awards_per_value', 'awards_per_state_value', 'awards_per_natl_value',\n","       'exp_award_value', 'exp_award_state_value', 'exp_award_natl_value',\n","       'exp_award_percentile', 'ft_pct', 'fte_value', 'fte_percentile',\n","       'med_sat_value', 'med_sat_percentile', 'aid_value', 'aid_percentile',\n","       'endow_value', 'endow_percentile', 'grad_100_value',\n","       'grad_100_percentile', 'grad_150_value', 'grad_150_percentile',\n","       'pell_value', 'pell_percentile', 'retain_value', 'retain_percentile',\n","       'ft_fac_value', 'ft_fac_percentile', 'vsa_year',\n","       'vsa_grad_after4_first', 'vsa_grad_elsewhere_after4_first',\n","       'vsa_enroll_after4_first', 'vsa_enroll_elsewhere_after4_first',\n","       'vsa_grad_after6_first', 'vsa_grad_elsewhere_after6_first',\n","       'vsa_enroll_after6_first', 'vsa_enroll_elsewhere_after6_first',\n","       'vsa_grad_after4_transfer', 'vsa_grad_elsewhere_after4_transfer',\n","       'vsa_enroll_after4_transfer', 'vsa_enroll_elsewhere_after4_transfer',\n","       'vsa_grad_after6_transfer', 'vsa_grad_elsewhere_after6_transfer',\n","       'vsa_enroll_after6_transfer', 'vsa_enroll_elsewhere_after6_transfer',\n","       'similar', 'state_sector_ct', 'carnegie_ct', 'counted_pct', 'nicknames',\n","       'cohort_size'],\n","      dtype='object')"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"a-XFGRZqTeWp"},"source":["# Selecting Columns of Interest.\n","cc_institution_details = cc_institution_details[['unitid', 'chronname', 'city', 'state', 'level', 'hbcu',\n","                                  'long_x', 'lat_y', 'site', 'student_count', 'med_sat_value', \n","                                  'aid_value', 'endow_value', ]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umO2jCRlUvCH","executionInfo":{"status":"ok","timestamp":1630963304425,"user_tz":240,"elapsed":4,"user":{"displayName":"Jonathan Schlosser","photoUrl":"","userId":"03711695131991659126"}},"outputId":"98b1a8aa-ce00-46fa-ba2d-05f9ffe1e2bd"},"source":["# Checking columns of our second dataset.\n","cc_institution_grads.columns\n","\n","# Since this dataset has already been prepared, we do not need to select out any columns because it has already been done. "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['unitid', 'Other', 'Female', 'Male', 'Asian', 'American Indian',\n","       'Black', 'Hispanic', 'White', 'Unknown'],\n","      dtype='object')"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"9LS-0s1CWbJq"},"source":["# Checking for Nulls\n","\n","Now that we have our columns of interest, we can check for missing data. One way to do this is to use the **isnull()** function and the **sum()** function built within Pandas. This will give us a list of missing values for each column in our dataset. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WhK_yz_hWbqM","executionInfo":{"status":"ok","timestamp":1631040084267,"user_tz":240,"elapsed":312,"user":{"displayName":"John Kirollos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXlpWNgQGE88mhrSgUJB6Dx3RLSlEhHifAZALdwg=s64","userId":"10752091771659842492"}},"outputId":"84f1d8d2-a27f-46d6-a7ea-142c5d8032fa"},"source":["cc_institution_details.isnull().sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["unitid              0\n","chronname           0\n","city                0\n","state               0\n","level               0\n","hbcu             1251\n","long_x              0\n","lat_y               0\n","site                0\n","student_count       0\n","med_sat_value       0\n","aid_value           0\n","endow_value         0\n","dtype: int64"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"okeCKN-HYdRN"},"source":["### Try it on your own!\n","\n","Try to identify the number of nulls in our second dataframe **cc_institution_grads**."]},{"cell_type":"code","metadata":{"id":"K6GbWPe4m0Zg"},"source":["# Please enter your solution here. "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-mo4Sx1GWa2L","cellView":"form"},"source":["#@title Solution is hidden. Double click to see it.\n","\n","cc_institution_grads.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bqgSGoAIcpE8"},"source":["# Dropping Nulls\n","\n","Now that we have checked our data, we can see that there are missing values within our first dataset, so we should probably clean that up. For today's case, we are just going to remove any null values. We want to remove the entire observation, or row, so that they do not affect our analyses later on. There are a number of other ways to address missing data including interpolation and extrapolation. \n","\n","To make sure that we are only removing data that needs to be removed, and that are not removing data will-nilly, we will want to identify particular columns. This prevents us from removing too much data. For example, we may not want to remove data just because it is missing an HBCU status or a website url, but we would want to remove observations if they are missing a student count or a median SAT value. So, we select the columns to check to make sure we are not dropping things unecessarily. \n","\n","Lastly, we also want to use the **inplace** parameter to make changes to our existing dataset. The default value for this is False. If we have this set to false, we need to assign the returned dataframe to a value otherwise the updated version would not be preserved. "]},{"cell_type":"code","metadata":{"id":"50EwMeO669uv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631040097499,"user_tz":240,"elapsed":225,"user":{"displayName":"John Kirollos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXlpWNgQGE88mhrSgUJB6Dx3RLSlEhHifAZALdwg=s64","userId":"10752091771659842492"}},"outputId":"04d5545a-db51-4f70-bb4a-f71887d6141a"},"source":["cc_institution_details.dropna(subset=['student_count', 'med_sat_value', 'aid_value', 'endow_value'], inplace=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"Entry point for launching an IPython kernel.\n"]}]},{"cell_type":"markdown","metadata":{"id":"9fi7M1cdVZ9d"},"source":["# Joining Datasets\n","\n","Now that we have our datasets cleaned up, we can work to merge them together. To do this, we use the **merge()** function that is built within Pandas. We specify the data to be joined, the type of join that we will be doing, and the column(s) that match the two dataframes. Including all of this information is generally considered good practice. \n","\n","Now, there are a number of different joins that we could do. The chart below shows them. For this instance, we want to preserve all of the schools, and thier respective information, in our dataset, rather than preserving all of the data on graduates. Therefore, we will be doing a left join, attaching our grads information onto our details information. \n","\n","![](https://i1.wp.com/datascienceexamples.com/wp-content/uploads/2019/12/join_types.png?fit=840%2C788&ssl=1)"]},{"cell_type":"code","metadata":{"id":"uSpP2zPrVZaU"},"source":["combined_df = cc_institution_details.merge(cc_institution_grads, how='left', on='unitid')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":486},"id":"J29spuFhVZIa","executionInfo":{"status":"ok","timestamp":1631040187002,"user_tz":240,"elapsed":18,"user":{"displayName":"John Kirollos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXlpWNgQGE88mhrSgUJB6Dx3RLSlEhHifAZALdwg=s64","userId":"10752091771659842492"}},"outputId":"f6a92c0d-c97c-48f8-c96e-c4809e8f246d"},"source":["combined_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>unitid</th>\n","      <th>chronname</th>\n","      <th>city</th>\n","      <th>state</th>\n","      <th>level</th>\n","      <th>hbcu</th>\n","      <th>long_x</th>\n","      <th>lat_y</th>\n","      <th>site</th>\n","      <th>student_count</th>\n","      <th>med_sat_value</th>\n","      <th>aid_value</th>\n","      <th>endow_value</th>\n","      <th>year</th>\n","      <th>gender</th>\n","      <th>race</th>\n","      <th>cohort</th>\n","      <th>grad_cohort</th>\n","      <th>grad_100</th>\n","      <th>grad_150</th>\n","      <th>grad_100_rate</th>\n","      <th>grad_150_rate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>100663</td>\n","      <td>University of Alabama at Birmingham</td>\n","      <td>Birmingham</td>\n","      <td>Alabama</td>\n","      <td>4-year</td>\n","      <td>NaN</td>\n","      <td>-86.80917</td>\n","      <td>33.50223</td>\n","      <td>www.uab.edu</td>\n","      <td>11502</td>\n","      <td>1146.0</td>\n","      <td>6088.0</td>\n","      <td>24136.0</td>\n","      <td>2011</td>\n","      <td>B</td>\n","      <td>X</td>\n","      <td>4y other</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>100.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>100663</td>\n","      <td>University of Alabama at Birmingham</td>\n","      <td>Birmingham</td>\n","      <td>Alabama</td>\n","      <td>4-year</td>\n","      <td>NaN</td>\n","      <td>-86.80917</td>\n","      <td>33.50223</td>\n","      <td>www.uab.edu</td>\n","      <td>11502</td>\n","      <td>1146.0</td>\n","      <td>6088.0</td>\n","      <td>24136.0</td>\n","      <td>2011</td>\n","      <td>M</td>\n","      <td>X</td>\n","      <td>4y other</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>100663</td>\n","      <td>University of Alabama at Birmingham</td>\n","      <td>Birmingham</td>\n","      <td>Alabama</td>\n","      <td>4-year</td>\n","      <td>NaN</td>\n","      <td>-86.80917</td>\n","      <td>33.50223</td>\n","      <td>www.uab.edu</td>\n","      <td>11502</td>\n","      <td>1146.0</td>\n","      <td>6088.0</td>\n","      <td>24136.0</td>\n","      <td>2011</td>\n","      <td>F</td>\n","      <td>X</td>\n","      <td>4y other</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>100.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>100663</td>\n","      <td>University of Alabama at Birmingham</td>\n","      <td>Birmingham</td>\n","      <td>Alabama</td>\n","      <td>4-year</td>\n","      <td>NaN</td>\n","      <td>-86.80917</td>\n","      <td>33.50223</td>\n","      <td>www.uab.edu</td>\n","      <td>11502</td>\n","      <td>1146.0</td>\n","      <td>6088.0</td>\n","      <td>24136.0</td>\n","      <td>2011</td>\n","      <td>B</td>\n","      <td>W</td>\n","      <td>4y other</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>100.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>100663</td>\n","      <td>University of Alabama at Birmingham</td>\n","      <td>Birmingham</td>\n","      <td>Alabama</td>\n","      <td>4-year</td>\n","      <td>NaN</td>\n","      <td>-86.80917</td>\n","      <td>33.50223</td>\n","      <td>www.uab.edu</td>\n","      <td>11502</td>\n","      <td>1146.0</td>\n","      <td>6088.0</td>\n","      <td>24136.0</td>\n","      <td>2011</td>\n","      <td>M</td>\n","      <td>W</td>\n","      <td>4y other</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   unitid                            chronname  ... grad_100_rate grad_150_rate\n","0  100663  University of Alabama at Birmingham  ...           NaN         100.0\n","1  100663  University of Alabama at Birmingham  ...           NaN           NaN\n","2  100663  University of Alabama at Birmingham  ...           NaN         100.0\n","3  100663  University of Alabama at Birmingham  ...           NaN         100.0\n","4  100663  University of Alabama at Birmingham  ...           NaN           NaN\n","\n","[5 rows x 22 columns]"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"3czQzIOWAbgG"},"source":["# Min, Max, Median\n","\n","Now that we have our data combined, we can begin looking at some descriptive statistics and getting a feel for our dataset. First, lets look at the min, max, and median using the functions **min()**, **max()**, and **median()**. These are super simple statistics but are very often used, so its good to be familiar with these functions and their use. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DilnLeq6lBSG","executionInfo":{"status":"ok","timestamp":1630963319777,"user_tz":240,"elapsed":219,"user":{"displayName":"Jonathan Schlosser","photoUrl":"","userId":"03711695131991659126"}},"outputId":"21cd947e-20e3-4ef0-e807-72246a3053ea"},"source":["min = combined_df['student_count'].min()\n","print(\"Min:\", min)\n","\n","max = combined_df['student_count'].max()\n","print(\"Max:\", max)\n","\n","median = combined_df['student_count'].median()\n","print(\"Median:\", median)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Min: 85\n","Max: 51333\n","Median: 2625.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"68hVjHYKlqAI","executionInfo":{"status":"ok","timestamp":1630963321593,"user_tz":240,"elapsed":207,"user":{"displayName":"Jonathan Schlosser","photoUrl":"","userId":"03711695131991659126"}},"outputId":"3280a60c-0f45-4317-fa65-de2e3fdb8ce3"},"source":["min = combined_df['med_sat_value'].min()\n","print(\"Min:\", min)\n","\n","max = combined_df['med_sat_value'].max()\n","print(\"Max:\", max)\n","\n","median = combined_df['med_sat_value'].median()\n","print(\"Median:\", median)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Min: 666.0\n","Max: 1534.0\n","Median: 1044.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"ErMcapVTlxiA"},"source":["### Try it on your own!\n","\n","Try to identify the min, max, and median for the columns **aid_value** and **endow_value**."]},{"cell_type":"code","metadata":{"id":"4yNiureQBPtw"},"source":["# Please enter your solution here."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aTKkIDwUAbKu","cellView":"form"},"source":["#@title Solution is hidden. Double click to see it.\n","\n","aid_min = combined_df['aid_value'].min()\n","print(\"Aid Min:\", aid_min)\n","\n","aid_max = combined_df['aid_value'].max()\n","print(\"Aid Max:\", aid_max)\n","\n","aid_median = combined_df['aid_value'].median()\n","print(\"Aid Median:\", aid_median)\n","\n","endow_min = combined_df['endow_value'].min()\n","print(\"Endowment Min:\", endow_min)\n","\n","endow_max = combined_df['endow_value'].max()\n","print(\"Endowment Max:\", endow_max)\n","\n","endow_median = combined_df['endow_value'].median()\n","print(\"Endowment Median:\", endow_median)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E-L-FMtHBeCU"},"source":["# Sum and Average\n","\n","Next, we can look at the sum and average using the functions **sum()** and **mean()**. Again, these are incredibly useful functions because they are very commonly employed. "]},{"cell_type":"code","metadata":{"id":"MrSJOQaHBsDy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630963325960,"user_tz":240,"elapsed":220,"user":{"displayName":"Jonathan Schlosser","photoUrl":"","userId":"03711695131991659126"}},"outputId":"9ec723db-5e60-4b42-e1fc-32cadfe17bb9"},"source":["sum = combined_df['student_count'].sum()\n","print(\"Sum:\", sum)\n","\n","average = combined_df['student_count'].mean()\n","print(\"Mean:\", average)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sum: 7500073\n","Mean: 5756.003837298542\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7oEm46-mnRGj","executionInfo":{"status":"ok","timestamp":1630963328219,"user_tz":240,"elapsed":7,"user":{"displayName":"Jonathan Schlosser","photoUrl":"","userId":"03711695131991659126"}},"outputId":"2c9495e6-d1e3-48a2-dee5-dd929e2095f9"},"source":["sum = combined_df['med_sat_value'].sum()\n","print(\"Sum:\", sum)\n","\n","average = combined_df['med_sat_value'].mean()\n","print(\"Mean:\", average)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sum: 1384325.0\n","Mean: 1062.4136607828088\n"]}]},{"cell_type":"markdown","metadata":{"id":"76BY3c4zBvk9"},"source":["### Try it on your own!\n","\n","Try to identify the sum and average for the columns **aid_value** and **endow_value**."]},{"cell_type":"code","metadata":{"id":"v1JuqyrFBuzB"},"source":["# Please enter your solution here."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HmKK45AiB3Q4","cellView":"form"},"source":["#@title Solution is hidden. Double click to see it.\n","\n","aid_sum = combined_df['aid_value'].sum()\n","print(\"Aid Sum:\", aid_sum)\n","\n","aid_average = combined_df['aid_value'].mean()\n","print(\"Aid Mean:\", aid_average)\n","\n","endow_sum = combined_df['endow_value'].sum()\n","print(\"Endowment Sum:\", endow_sum)\n","\n","endow_average = combined_df['endow_value'].mean()\n","print(\"Endowment Mean:\", endow_average)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"piuJ2c34CGJF"},"source":["# Standard Deviation and Quantiles\n","\n","\n","Lastly, we can take a look at the standard deviation and some initial quantile values. To do this we use the function **std()** to find the standard deviation, and the funtion **quantile()** to identify a quantile amount. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u__5g6owtwRB","executionInfo":{"status":"ok","timestamp":1630963331667,"user_tz":240,"elapsed":4,"user":{"displayName":"Jonathan Schlosser","photoUrl":"","userId":"03711695131991659126"}},"outputId":"2bc09daf-f5dd-43ed-9172-51397408a03f"},"source":["std = combined_df['student_count'].std()\n","print(\"Standard Deviation:\", std)\n","\n","quant25 = combined_df['student_count'].quantile(.25)\n","print(\".25 Quantile:\", quant25)\n","\n","quant75= combined_df['student_count'].quantile(.75)\n","print(\".75 Quantile:\", quant75)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Standard Deviation: 7403.819110958123\n",".25 Quantile: 1360.0\n",".75 Quantile: 6615.5\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AWberL6TuUg1","executionInfo":{"status":"ok","timestamp":1630963335140,"user_tz":240,"elapsed":203,"user":{"displayName":"Jonathan Schlosser","photoUrl":"","userId":"03711695131991659126"}},"outputId":"8a8a0093-cd32-4a9b-f05e-ef3fa2a35c74"},"source":["std = combined_df['med_sat_value'].std()\n","print(\"Standard Deviation:\", std)\n","\n","quant25 = combined_df['med_sat_value'].quantile(.25)\n","print(\".25 Quantile:\", quant25)\n","\n","quant75= combined_df['med_sat_value'].quantile(.75)\n","print(\".75 Quantile:\", quant75)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Standard Deviation: 132.4306445062105\n",".25 Quantile: 976.0\n",".75 Quantile: 1125.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"kcYTvEE3uey5"},"source":["### Try it on your own!\n","\n","Try to identify the standard deviation, the 25th Quantile, and the 75th Quantile for the columns **aid_value** and **endow_value**."]},{"cell_type":"code","metadata":{"id":"HRZH0YOjufN9"},"source":["# Please enter your solution here."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FBCt7KwruuWU","cellView":"form"},"source":["#@title Solution is hidden. Double click to see it.\n","\n","aid_std = combined_df['aid_value'].std()\n","print(\"Aid Standard Deviation:\", aid_std)\n","\n","aid_quant25 = combined_df['aid_value'].quantile(.25)\n","print(\"Aid .25 Quantile:\", aid_quant25)\n","\n","aid_quant75= combined_df['aid_value'].quantile(.75)\n","print(\"Aid .75 Quantile:\", aid_quant75)\n","\n","endow_std = combined_df['endow_value'].std()\n","print(\"Endowment Standard Deviation:\", endow_std)\n","\n","endow_quant25 = combined_df['endow_value'].quantile(.25)\n","print(\"Endowment .25 Quantile:\", endow_quant25)\n","\n","endow_quant75= combined_df['endow_value'].quantile(.75)\n","print(\"Endowment .75 Quantile:\", endow_quant75)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B63_kP9QcMTu"},"source":["# Basic Statistics\n","\n","Now that we have gone through all of these initial descriptive statistics, we can highlight an easier way to get all of this information. There is a handy function within Pandas called **describe()** that gets basic statistics for all of the numerical columns in our dataframe. But, if we want to include all of the columns, and not just the numeric ones, we need to adjust the code a little bit by specifying **include='all'**. "]},{"cell_type":"code","metadata":{"id":"4aIK0de8a3FM","colab":{"base_uri":"https://localhost:8080/","height":334},"executionInfo":{"status":"ok","timestamp":1630963339958,"user_tz":240,"elapsed":5,"user":{"displayName":"Jonathan Schlosser","photoUrl":"","userId":"03711695131991659126"}},"outputId":"1ef6109a-6f96-4132-ee4e-f9de840f955b"},"source":["combined_df.describe()#include='all'"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>unitid</th>\n","      <th>long_x</th>\n","      <th>lat_y</th>\n","      <th>student_count</th>\n","      <th>med_sat_value</th>\n","      <th>aid_value</th>\n","      <th>endow_value</th>\n","      <th>Other</th>\n","      <th>Female</th>\n","      <th>Male</th>\n","      <th>Asian</th>\n","      <th>American Indian</th>\n","      <th>Black</th>\n","      <th>Hispanic</th>\n","      <th>White</th>\n","      <th>Unknown</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>1303.000000</td>\n","      <td>1303.000000</td>\n","      <td>1303.000000</td>\n","      <td>1303.000000</td>\n","      <td>1303.000000</td>\n","      <td>1303.000000</td>\n","      <td>1.303000e+03</td>\n","      <td>1303.000000</td>\n","      <td>1303.000000</td>\n","      <td>1303.000000</td>\n","      <td>1303.000000</td>\n","      <td>1303.000000</td>\n","      <td>1303.000000</td>\n","      <td>1303.000000</td>\n","      <td>1303.000000</td>\n","      <td>1303.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>184271.828856</td>\n","      <td>-88.185118</td>\n","      <td>38.527853</td>\n","      <td>5756.003837</td>\n","      <td>1062.413661</td>\n","      <td>12875.875672</td>\n","      <td>4.938504e+04</td>\n","      <td>141.872602</td>\n","      <td>141.872602</td>\n","      <td>141.872602</td>\n","      <td>70.936301</td>\n","      <td>70.936301</td>\n","      <td>70.936301</td>\n","      <td>70.936301</td>\n","      <td>70.936301</td>\n","      <td>70.936301</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>46942.878399</td>\n","      <td>14.432181</td>\n","      <td>4.643846</td>\n","      <td>7403.819111</td>\n","      <td>132.430645</td>\n","      <td>7516.011605</td>\n","      <td>1.570608e+05</td>\n","      <td>13.364983</td>\n","      <td>13.364983</td>\n","      <td>13.364983</td>\n","      <td>6.682492</td>\n","      <td>6.682492</td>\n","      <td>6.682492</td>\n","      <td>6.682492</td>\n","      <td>6.682492</td>\n","      <td>6.682492</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>100663.000000</td>\n","      <td>-158.062444</td>\n","      <td>19.701854</td>\n","      <td>85.000000</td>\n","      <td>666.000000</td>\n","      <td>1184.000000</td>\n","      <td>8.000000e+00</td>\n","      <td>36.000000</td>\n","      <td>36.000000</td>\n","      <td>36.000000</td>\n","      <td>18.000000</td>\n","      <td>18.000000</td>\n","      <td>18.000000</td>\n","      <td>18.000000</td>\n","      <td>18.000000</td>\n","      <td>18.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>152548.500000</td>\n","      <td>-93.948411</td>\n","      <td>35.194854</td>\n","      <td>1360.000000</td>\n","      <td>976.000000</td>\n","      <td>6763.500000</td>\n","      <td>4.335500e+03</td>\n","      <td>144.000000</td>\n","      <td>144.000000</td>\n","      <td>144.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>185572.000000</td>\n","      <td>-84.554657</td>\n","      <td>39.709671</td>\n","      <td>2625.000000</td>\n","      <td>1044.000000</td>\n","      <td>11409.000000</td>\n","      <td>1.147400e+04</td>\n","      <td>144.000000</td>\n","      <td>144.000000</td>\n","      <td>144.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>214781.500000</td>\n","      <td>-77.644559</td>\n","      <td>41.738872</td>\n","      <td>6615.500000</td>\n","      <td>1125.000000</td>\n","      <td>16873.500000</td>\n","      <td>3.266800e+04</td>\n","      <td>144.000000</td>\n","      <td>144.000000</td>\n","      <td>144.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>462354.000000</td>\n","      <td>-67.456534</td>\n","      <td>61.191235</td>\n","      <td>51333.000000</td>\n","      <td>1534.000000</td>\n","      <td>41580.000000</td>\n","      <td>2.505435e+06</td>\n","      <td>144.000000</td>\n","      <td>144.000000</td>\n","      <td>144.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","      <td>72.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              unitid       long_x  ...        White      Unknown\n","count    1303.000000  1303.000000  ...  1303.000000  1303.000000\n","mean   184271.828856   -88.185118  ...    70.936301    70.936301\n","std     46942.878399    14.432181  ...     6.682492     6.682492\n","min    100663.000000  -158.062444  ...    18.000000    18.000000\n","25%    152548.500000   -93.948411  ...    72.000000    72.000000\n","50%    185572.000000   -84.554657  ...    72.000000    72.000000\n","75%    214781.500000   -77.644559  ...    72.000000    72.000000\n","max    462354.000000   -67.456534  ...    72.000000    72.000000\n","\n","[8 rows x 16 columns]"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"2ZxVIUBI5xUe"},"source":["### Try it on your own!\n","\n","Try to describe only the four columns that we have previously been working with, rather than the whole table. "]},{"cell_type":"code","metadata":{"id":"zwRTwd4k5_G8"},"source":["# Please enter your solution here."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nwLk5VX1cDEX","cellView":"form"},"source":["#@title Solution is hidden. Double click to see it.\n","combined_df[['student_count', 'med_sat_value',\n","             'aid_value', 'endow_value']].describe()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7A-UdElt-ezo"},"source":["# Saving Files\n","\n","Lastly, we want to save our combined dataframe so that we can use it for future analyses. To do this, we use the function to_csv(). Please fill in the data path below and save the file some place where you will ba able to locate it in the future. "]},{"cell_type":"code","metadata":{"id":"Cayk2Pss-dHg","colab":{"base_uri":"https://localhost:8080/","height":167},"executionInfo":{"status":"error","timestamp":1631063972959,"user_tz":240,"elapsed":223,"user":{"displayName":"John Kirollos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhXlpWNgQGE88mhrSgUJB6Dx3RLSlEhHifAZALdwg=s64","userId":"10752091771659842492"}},"outputId":"fbea0146-818b-483c-8074-badaf79cc35e"},"source":["combined_df.to_csv('/###Adjust Path Here###/combined_df.csv', index=False) # Adjust File Path As Needed!"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-4ef249a59006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombined_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/combined_df.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Adjust File Path As Needed!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'combined_df' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"bp9ClE8kwS-5"},"source":["# Conclusions\n","\n","This case introduced us to some basic Pandas concepts, but this is just a start. Pandas is incredibly powerful and innumerable applications. Data Science and Pandas go hand in hand and is crucial to a data science career today. \n","\n","In our next lesson, we will be continuing with this case and learning some more advanced techniques and approaches. Please come and check that out if you would like to go further and learn some more information. \n","\n","If you would like more information on Pandas, please check out the [documentation](https://pandas.pydata.org/docs/reference/index.html) which outlines all of the Pandas functions and their uses. "]}]}